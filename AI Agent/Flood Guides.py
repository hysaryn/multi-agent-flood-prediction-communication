# -*- coding: utf-8 -*-
"""Flood Guides.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NdvXn5UMX5NDDplSUXIi-wRmDpugXhNX
"""

import subprocess
import sys

# Install required packages
subprocess.run([sys.executable, "-m", "pip", "install", "python-dotenv", "huggingface-hub", "llama-index", "transformers", "sentence-transformers", "llama-index-llms-huggingface", "llama-index-embeddings-huggingface", "pdfplumber", "llama-index-llms-openrouter", "llama-index-retrievers-bm25", "tabula-py", "jpype1", "pystemmer"])

"""## 1. Setup & Configuration
- Import required Python libraries.
- Load API keys securely from environment variables.
- Initialize the OpenRouter LLMs for both querying and evaluation.
- Set up the HuggingFace embedding model for text representation.
- Apply `nest_asyncio` to handle event loop issues in Jupyter environments.

"""

import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # Suppresses TensorFlow warnings
from dotenv import load_dotenv
import Stemmer
import nest_asyncio
import tabula
import pandas as pd
from dotenv import load_dotenv
from llama_index.core import Document
from llama_index.core import (SimpleDirectoryReader, VectorStoreIndex, Settings)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openrouter import OpenRouter
from llama_index.embeddings.openai import OpenAIEmbedding
import asyncio


load_dotenv()  # Load environment variables from .env file


# Load API Key Securely (No Hardcoding!)
api_key = os.getenv("OPENROUTER_API_KEY")
if api_key:
    print("âœ… API Key Loaded Successfully:", api_key[:5] + "..." + api_key[-5:])
else:
    print("âš ï¸ API Key is missing! Check your .env file.")

# Initialize OpenRouter LLM
llm = OpenRouter(
    api_key=api_key,
    api_base="https://openrouter.ai/api/v1",    
    model="mistralai/mistral-7b-instruct",
    max_tokens=512,
    context_window=4096
)
Judge_llm = OpenRouter(
    api_key=api_key,
    api_base="https://openrouter.ai/api/v1",
    model="qwen/qwen-turbo",
    max_tokens=512,
    context_window=4096
)
Settings.llm = llm

# Apply nest_asyncio to fix event loop issues in Jupyter
nest_asyncio.apply()

# Set up embedding model
embed_model_name = "sentence-transformers/all-MiniLM-L6-v2"
embed_model = HuggingFaceEmbedding(model_name=embed_model_name)
Settings.embed_model = embed_model

"""### 2. Document Loading & Preprocessing
- Read the documents and extract its textual content.
- Extract tabular data from the PDF using `tabula`.
- Convert the extracted table data into text format.
- Combine the extracted text and tables into a unified document.
- Define an ingestion pipeline to preprocess text by splitting it into manageable chunks and applying embeddings.

"""

import warnings
import logging
warnings.filterwarnings("ignore", category=UserWarning, module="pypdf")
warnings.filterwarnings("ignore", category=UserWarning, module="pdfplumber")
logging.getLogger("pdfminer").setLevel(logging.ERROR)


print("ðŸ“ Loading PDF documents...")
pdf_paths = [
  "Alert Guides Docs/Prepared BC Flood Preparedness Guide.pdf",
  "Alert Guides Docs/Farm Flood Readiness Toolkit.pdf",
  "Alert Guides Docs/Provincial Flood Emergency Plan.pdf"]
documents = SimpleDirectoryReader(input_files=pdf_paths).load_data()
print(f"âœ… Loaded {len(documents)} documents")

# Convert table data into an additional Document
print("ðŸ“Š Extracting tables from PDFs...")
tables = []
for i, f in enumerate(pdf_paths, 1):
    print(f"  Processing PDF {i}/{len(pdf_paths)}: {f}")
    try:
        dfs = tabula.read_pdf(f, pages="all", multiple_tables=True)  # multiple_tables=True æ›´ç¨³
        tables.extend(dfs)
        print(f"    âœ… Found {len(dfs)} tables")
    except Exception as e:
        print(f"    âš ï¸ Warning: could not read tables from {f}: {e}")
table_docs = [df.to_markdown(index=False) for df in tables]
all_tables_text = "\n\n".join(table_docs)
document_from_tables = Document(text=all_tables_text)
print(f"âœ… Total tables extracted: {len(tables)}")


# Combine original + table doc
documents = documents + [document_from_tables]

# Create the pipeline with transformations
print("ðŸ”§ Setting up text processing pipeline...")
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.ingestion import IngestionPipeline

pipeline = IngestionPipeline(transformations=[SentenceSplitter(chunk_size=800, chunk_overlap=80), embed_model])

# Run the pipeline
print("âš™ï¸ Processing documents into chunks...")
nodes = pipeline.run(documents=documents)
print(f"âœ… Created {len(nodes)} text chunks")

"""### 3. Indexing & Retrieval
- Build a `VectorStoreIndex` from the preprocessed document chunks.
- Implement different retrieval methods:
  - **Base Retriever:** Retrieves the most relevant document chunk.
  - **AutoMerging Retriever:** Aggregates multiple related chunks before returning results.
  - **BM25 Retriever:** Uses term frequency-based ranking (a traditional information retrieval method).
  - **Hybrid Fusion Retriever:** (Task 4)
- Define retrieval hyperparameters, such as similarity threshold and number of top results to return.

"""

# Create Vector Index and Query Engine
print("ðŸ” Building vector index...")
index = VectorStoreIndex(nodes)
print("âœ… Vector index created")

# Create base retrievers
print("ðŸ”§ Initializing retrievers...")
base_retriever = index.as_retriever(similarity_top_k=2)
print("âœ… Base Retriever initialized.")

from llama_index.core.retrievers import AutoMergingRetriever
auto_base_retriever = index.as_retriever(similarity_top_k=3)
auto_merging_retriever = AutoMergingRetriever(auto_base_retriever, index.storage_context)
print("âœ… Auto-Merging Retriever initialized.")

from llama_index.retrievers.bm25 import BM25Retriever
bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=2, stemmer=Stemmer.Stemmer("english"), language="english")
print("âœ… BM25 Retriever initialized.")

from llama_index.core.retrievers import QueryFusionRetriever

# âœ… Hybrid Fusion Retriever (Combining BM25 and Vector Retrieval)
hybrid_retriever = QueryFusionRetriever(
    retrievers=[auto_merging_retriever, bm25_retriever],
    retriever_weights=[0.8, 0.2],
    similarity_top_k=2
)
print("âœ… Hybrid Fusion Retriever initialized.")

"""### 4. Query Engines & Evaluation Setup
- Instantiate query engines for each retrieval method to allow direct querying.
- Define evaluation models to measure:
  - **Faithfulness:** Whether the retrieved information is accurate and grounded in the original document.
  - **Relevancy:** Whether the retrieved information is relevant to the query.
- Configure different retriever evaluators to assess retrieval effectiveness using metrics such as:
  - Mean Reciprocal Rank (MRR)
  - Hit Rate
  - Precision
  - Recall
"""

# âœ… Create query engines
from llama_index.core.query_engine import RetrieverQueryEngine

base_query_engine = RetrieverQueryEngine.from_args(base_retriever)
auto_query_engine = RetrieverQueryEngine.from_args(auto_merging_retriever)
bm25_query_engine = RetrieverQueryEngine.from_args(bm25_retriever)
hybrid_query_engine = RetrieverQueryEngine(retriever=hybrid_retriever)

# âœ… Initialize Evaluators
from llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator, RetrieverEvaluator

faithfulness_evaluator = FaithfulnessEvaluator(llm=Judge_llm)
relevancy_evaluator = RelevancyEvaluator(llm=Judge_llm)

# âœ… Define retriever evaluators
base_retriever_evaluator = RetrieverEvaluator.from_metric_names(["mrr", "hit_rate", "precision", "recall"], retriever=base_retriever)
auto_retriever_evaluator = RetrieverEvaluator.from_metric_names(["mrr", "hit_rate", "precision", "recall"], retriever=auto_merging_retriever)
bm25_retriever_evaluator = RetrieverEvaluator.from_metric_names(["mrr", "hit_rate", "precision", "recall"], retriever=bm25_retriever)
hybrid_retriever_evaluator = RetrieverEvaluator.from_metric_names(["mrr", "hit_rate", "precision", "recall"], retriever=hybrid_retriever)

"""### 5. Preparing Evaluation Questions & Display Functions

In this section, we define a set of evaluation questions to systematically assess the performance of different retrieval methods. Additionally, we create utility functions for displaying results in a structured and readable format.

Key Steps:
- Define a set of test queries (e.g., syllabus-related questions).
- Generate question-context pairs for automated evaluation.

To make evaluation results more interpretable, we define:
- **`displayify_df(df)`** â€“ A helper function to format and display DataFrames neatly in Jupyter notebooks.
- **`display_retriever_eval_results(name, eval_results)`** â€“ Computes and prints key retrieval evaluation metrics.
"""

# âœ… Use Evaluation Questions
eval_questions = []
with open('AI Agent/eval_questions.txt', 'r') as file:
    for line in file:
        eval_questions.append(line.strip())

# âœ… Add more evaluation questions
eval_questions += [
    "What essential items should be included in an evacuation go-bag for families?",
    "What steps should farmers take to protect livestock and equipment before heavy rain and flooding?"
]

from llama_index.core.evaluation import generate_question_context_pairs
qa_dataset = generate_question_context_pairs(nodes=nodes, llm=llm, num_questions_per_chunk=1)

# âœ… Pretty Display Function
def displayify_df(df):
    """For pretty displaying DataFrame in a notebook."""
    print("\n" + "="*100)
    print("ðŸ” EVALUATION RESULTS SUMMARY")
    print("="*100)
    
    # Display summary statistics first
    print("\nðŸ“Š PERFORMANCE COMPARISON")
    print("-" * 50)
    
    # Calculate and display mean scores
    faithfulness_cols = [col for col in df.columns if 'Faithfulness' in col]
    relevancy_cols = [col for col in df.columns if 'Relevancy' in col]
    
    print("\nðŸŽ¯ FAITHFULNESS SCORES (Higher is Better)")
    print("-" * 40)
    for col in faithfulness_cols:
        mean_score = df[col].mean()
        retriever_name = col.replace(' Faithfulness', '')
        print(f"{retriever_name:20}: {mean_score:.3f}")
    
    print("\nðŸŽ¯ RELEVANCY SCORES (Higher is Better)")
    print("-" * 40)
    for col in relevancy_cols:
        mean_score = df[col].mean()
        retriever_name = col.replace(' Relevancy', '')
        print(f"{retriever_name:20}: {mean_score:.3f}")
    
    print("\n" + "="*100)
    print("ðŸ“‹ DETAILED RESULTS BY QUESTION")
    print("="*100)
    
    # Display each question result separately
    for idx, row in df.iterrows():
        print(f"\nðŸ”¸ QUESTION {idx + 1}:")
        print("-" * 60)
        print(f"Query: {row['Query'][:100]}{'...' if len(str(row['Query'])) > 100 else ''}")
        
        print(f"\nðŸ“ RESPONSES:")
        response_cols = [col for col in df.columns if 'Response' in col]
        for col in response_cols:
            retriever_name = col.replace(' Response', '')
            response_text = str(row[col])[:150] + "..." if len(str(row[col])) > 150 else str(row[col])
            print(f"  {retriever_name:15}: {response_text}")
        
        print(f"\nðŸ“Š SCORES:")
        for col in faithfulness_cols + relevancy_cols:
            retriever_name = col.replace(' Faithfulness', '').replace(' Relevancy', '')
            score_type = 'Faithfulness' if 'Faithfulness' in col else 'Relevancy'
            score = row[col]
            print(f"  {retriever_name:15} {score_type:12}: {score:.3f}")
        
        print("-" * 60)
    
    print("\n" + "="*100)
    print("âœ… EVALUATION COMPLETE")
    print("="*100 + "\n")

# Helper to display retrieval metrics
def display_retriever_eval_results(name, eval_results):
    """Build a small DataFrame summarizing retrieval metrics across queries."""
    print(f"\nðŸ”§ {name.upper()} METRICS")
    print("=" * 50)
    metric_dicts = [res.metric_vals_dict for res in eval_results]
    if not metric_dicts:
        print("No retriever metrics found!")
        return
    df = pd.DataFrame(metric_dicts)
    
    # Display metrics in a more readable format
    print("\nðŸ“ˆ DETAILED METRICS:")
    for col in df.columns:
        if df[col].dtype in ['float64', 'int64']:
            mean_val = df[col].mean()
            std_val = df[col].std()
            print(f"  {col:20}: {mean_val:.3f} Â± {std_val:.3f}")
    
    print(f"\nðŸ“Š SUMMARY:")
    print(f"  Mean Values:")
    for col in df.columns:
        if df[col].dtype in ['float64', 'int64']:
            mean_val = df[col].mean()
            print(f"    {col:18}: {mean_val:.3f}")
    print()

"""### 6. Running Evaluations & Comparing Retrievers
- Execute retrieval on all retrievers (Base, AutoMerging, BM25, Hybrid Fusion).
- Assess each retriever's faithfulness and relevancy scores.
- Compare retrieval results by averaging the evaluation metrics.
- Display results in tabular format for easy comparison.

This step helps determine which retrieval method performs best for a given dataset.

"""

# âœ… Modify Evaluation to Include Fusion Retriever
async def run_evaluation():
    eval_results = []

    for query in eval_questions:
        # Retrieve responses from all four retrievers
        base_response = base_query_engine.query(query)
        auto_response = auto_query_engine.query(query)
        bm25_response = bm25_query_engine.query(query)
        hybrid_response = hybrid_query_engine.query(query)

        base_text = base_response.response
        auto_text = auto_response.response
        bm25_text = bm25_response.response
        hybrid_text = hybrid_response.response  # âœ… Hybrid Response

        base_contexts = "\n".join([node.get_content() for node in base_response.source_nodes])
        auto_contexts = "\n".join([node.get_content() for node in auto_response.source_nodes])
        bm25_contexts = "\n".join([node.get_content() for node in bm25_response.source_nodes])
        hybrid_contexts = "\n".join([node.get_content() for node in hybrid_response.source_nodes])  # âœ… Hybrid Context


        # Evaluate Faithfulness & Relevancy for each retriever
        base_faithfulness = faithfulness_evaluator.evaluate_response(response=base_response)
        auto_faithfulness = faithfulness_evaluator.evaluate_response(response=auto_response)
        bm25_faithfulness = faithfulness_evaluator.evaluate_response(response=bm25_response)
        hybrid_faithfulness = faithfulness_evaluator.evaluate_response(response=hybrid_response)  # âœ… Hybrid Evaluation

        base_relevancy = relevancy_evaluator.evaluate_response(query=query, response=base_response)
        auto_relevancy = relevancy_evaluator.evaluate_response(query=query, response=auto_response)
        bm25_relevancy = relevancy_evaluator.evaluate_response(query=query, response=bm25_response)
        hybrid_relevancy = relevancy_evaluator.evaluate_response(query=query, response=hybrid_response)  # âœ… Hybrid Evaluation

        eval_results.append({
            "Query": query,
            "Base Response": base_text,
            "Auto-Merged Response": auto_text,
            "BM25 Response": bm25_text,
            "Hybrid Response": hybrid_text,  # âœ… Added Hybrid Response
            "Base Context": "".join(base_contexts[:100]) + "... " + "".join(base_contexts[-100:]),
            "Auto Context": "".join(auto_contexts[:100]) + "... " + "".join(auto_contexts[-100:]),
            "BM25 Context": "".join(bm25_contexts[:100]) + "... " + "".join(bm25_contexts[-100:]),
            "Hybrid Context": "".join(hybrid_contexts[:100]) + "... " + "".join(hybrid_contexts[-100:]),  # âœ… Hybrid Context
            "Base Faithfulness": base_faithfulness.score,
            "Auto Faithfulness": auto_faithfulness.score,
            "BM25 Faithfulness": bm25_faithfulness.score,
            "Hybrid Faithfulness": hybrid_faithfulness.score,  # âœ… Hybrid Faithfulness
            "Base Relevancy": base_relevancy.score,
            "Auto Relevancy": auto_relevancy.score,
            "BM25 Relevancy": bm25_relevancy.score,
            "Hybrid Relevancy": hybrid_relevancy.score,  # âœ… Hybrid Relevancy
        })

    df = pd.DataFrame(eval_results)

    # Compute means for comparison
    print("\nâœ… Faithfulness Comparison")
    print(f"Base Retriever: {df['Base Faithfulness'].mean():.4f}")
    print(f"Auto-Merging Retriever: {df['Auto Faithfulness'].mean():.4f}")
    print(f"BM25 Retriever: {df['BM25 Faithfulness'].mean():.4f}")
    print(f"Hybrid Retriever: {df['Hybrid Faithfulness'].mean():.4f}")  # âœ… Hybrid Metrics

    print("\nâœ… Relevancy Comparison")
    print(f"Base Retriever: {df['Base Relevancy'].mean():.4f}")
    print(f"Auto-Merging Retriever: {df['Auto Relevancy'].mean():.4f}")
    print(f"BM25 Retriever: {df['BM25 Relevancy'].mean():.4f}")
    print(f"Hybrid Retriever: {df['Hybrid Relevancy'].mean():.4f}")  # âœ… Hybrid Metrics

    # Evaluate all retrievers on the QA dataset
    base_eval_results = await base_retriever_evaluator.aevaluate_dataset(qa_dataset)
    auto_eval_results = await auto_retriever_evaluator.aevaluate_dataset(qa_dataset)
    bm25_eval_results = await bm25_retriever_evaluator.aevaluate_dataset(qa_dataset)
    hybrid_eval_results = await hybrid_retriever_evaluator.aevaluate_dataset(qa_dataset)  # âœ… Hybrid Evaluation

    print("\n=== Retrieval Metrics Comparison ===")
    display_retriever_eval_results("Base Retriever", base_eval_results)
    display_retriever_eval_results("Auto-Merging Retriever", auto_eval_results)
    display_retriever_eval_results("BM25 Retriever", bm25_eval_results)
    display_retriever_eval_results("Hybrid Retriever", hybrid_eval_results)  # âœ… Hybrid Metrics

    # Display results table
    displayify_df(df)

# âœ… Execute Async Evaluation
print("ðŸš€ Starting evaluation process...")
asyncio.run(run_evaluation())
print("ðŸŽ‰ Evaluation completed!")